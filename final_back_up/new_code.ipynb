{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d922ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:/spark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41368fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraTest\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"172.22.213.208\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1ccb421",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o167.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.apache.spark.sql.cassandra. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.cassandra.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.sql.cassandra\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_interactions1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyspace\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mecommerce\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m df.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:314\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28mself\u001b[39m._jreader.load(\u001b[38;5;28mself\u001b[39m._spark._sc._jvm.PythonUtils.toSeq(path)))\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o167.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.apache.spark.sql.cassandra. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.cassandra.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"user_interactions1\", keyspace=\"ecommerce\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\"C:\\spark\\jars\\spark-cassandra-connector_2.12-3.5.0.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d8d874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraTest\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"172.22.213.208\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62e28ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o138.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.apache.spark.sql.cassandra. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.cassandra.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.sql.cassandra\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_interactions1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyspace\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mecommerce\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m df.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:314\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28mself\u001b[39m._jreader.load(\u001b[38;5;28mself\u001b[39m._spark._sc._jvm.PythonUtils.toSeq(path)))\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o138.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.apache.spark.sql.cassandra. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.cassandra.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"user_interactions1\", keyspace=\"ecommerce\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a86f85f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `action` cannot be resolved. Did you mean one of the following? [`Color`, `ID`, `Name`, `Price`, `Rating`].;\n'Project [ID#85, Name#86, Image#87, SubCategory#88, Category#89, Price#90, Brand#91, Description#92, Rating#93, Rating_Count#94, Stock_Quantity#95, Size#96, Color#97, Warranty#98, CASE WHEN ('action = view_details) THEN 1 WHEN ('action = like) THEN 2 WHEN ('action = add_to_cart) THEN 3 ELSE 0 END AS weight#170]\n+- Relation [ID#85,Name#86,Image#87,SubCategory#88,Category#89,Price#90,Brand#91,Description#92,Rating#93,Rating_Count#94,Stock_Quantity#95,Size#96,Color#97,Warranty#98] JDBCRelation(product_details) [numPartitions=1]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, when, \u001b[38;5;28msum\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m spark_sum\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Step 1: Add a weight column based on action type\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m weighted_df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mview_details\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlike\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madd_to_cart\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Step 2: Calculate user-product interaction score\u001b[39;00m\n\u001b[32m     13\u001b[39m user_product_score = weighted_df.groupBy(\u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m\"\u001b[39m).agg(\n\u001b[32m     14\u001b[39m     spark_sum(\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33minteraction_score\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5176\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   5171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   5172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   5173\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5174\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   5175\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `action` cannot be resolved. Did you mean one of the following? [`Color`, `ID`, `Name`, `Price`, `Rating`].;\n'Project [ID#85, Name#86, Image#87, SubCategory#88, Category#89, Price#90, Brand#91, Description#92, Rating#93, Rating_Count#94, Stock_Quantity#95, Size#96, Color#97, Warranty#98, CASE WHEN ('action = view_details) THEN 1 WHEN ('action = like) THEN 2 WHEN ('action = add_to_cart) THEN 3 ELSE 0 END AS weight#170]\n+- Relation [ID#85,Name#86,Image#87,SubCategory#88,Category#89,Price#90,Brand#91,Description#92,Rating#93,Rating_Count#94,Stock_Quantity#95,Size#96,Color#97,Warranty#98] JDBCRelation(product_details) [numPartitions=1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, sum as spark_sum\n",
    "\n",
    "# Step 1: Add a weight column based on action type\n",
    "weighted_df = df.withColumn(\n",
    "    \"weight\",\n",
    "    when(col(\"action\") == \"view_details\", 1)\n",
    "    .when(col(\"action\") == \"like\", 2)\n",
    "    .when(col(\"action\") == \"add_to_cart\", 3)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Step 2: Calculate user-product interaction score\n",
    "user_product_score = weighted_df.groupBy(\"user_id\", \"product_id\").agg(\n",
    "    spark_sum(\"weight\").alias(\"interaction_score\")\n",
    ")\n",
    "\n",
    "# Step 3: Create a global popularity score for products\n",
    "global_product_score = weighted_df.groupBy(\"product_id\").agg(\n",
    "    spark_sum(\"weight\").alias(\"popularity_score\")\n",
    ")\n",
    "\n",
    "# Step 4: Get products the user has already interacted with\n",
    "interacted_products = df.select(\"user_id\", \"product_id\").distinct()\n",
    "\n",
    "# Step 5: Recommend top popular products that the user has NOT interacted with\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "user_id = 1  # Set your target user ID here\n",
    "\n",
    "# Filter interacted products for the user\n",
    "user_interacted = interacted_products.filter(col(\"user_id\") == user_id)\n",
    "\n",
    "# Join to exclude already seen products\n",
    "recommendations = global_product_score.join(\n",
    "    broadcast(user_interacted), on=\"product_id\", how=\"left_anti\"\n",
    ")\n",
    "\n",
    "# Get top N recommendations\n",
    "top_recommendations = recommendations.orderBy(col(\"popularity_score\").desc()).limit(10)\n",
    "\n",
    "top_recommendations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6afad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Using the --packages option\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraTest\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"172.22.213.208\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading from Cassandra\n",
    "try:\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(table=\"user_interactions1\", keyspace=\"ecommerce\") \\\n",
    "        .load()\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a4b9944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector()\n",
      "Error: An error occurred while calling o237.load.\n",
      ": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.apache.spark.sql.cassandra. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.cassandra.DefaultSource\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n",
      "\tat scala.util.Failure.orElse(Try.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n",
      "\t... 14 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Path to the Cassandra connector JAR file\n",
    "connector_jar_path = \"C:\\\\jdbccas\\\\spark-cassandra-connector-assembly_2.12-3.5.0.jar\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraTest\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"172.22.213.208\") \\\n",
    "    .config(\"spark.jars\", connector_jar_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify the loaded JARs\n",
    "print(spark.sparkContext._jsc.sc().listJars())\n",
    "\n",
    "# Attempt to read data from Cassandra\n",
    "try:\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(table=\"user_interactions1\", keyspace=\"ecommerce\") \\\n",
    "        .load()\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04cad3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(spark://localhost:50249/jars/mysql-connector-j-8.4.0.jar)\n",
      "+-------+--------------------+-----------+----------------+----------+\n",
      "|user_id|    action_timestamp|     action|        category|product_id|\n",
      "+-------+--------------------+-----------+----------------+----------+\n",
      "|      2|2025-04-17 07:31:...|       like|     Electronics|      1019|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|     Electronics|      1019|\n",
      "|      2|2025-04-17 07:31:...|       like|          Gaming|      1018|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|          Gaming|      1018|\n",
      "|      2|2025-04-17 07:31:...|       like|  Home & Kitchen|      1017|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|  Home & Kitchen|      1017|\n",
      "|      2|2025-04-17 07:31:...|       like|   Mobile Phones|      1016|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|   Mobile Phones|      1016|\n",
      "|      2|2025-04-17 07:31:...|       like|         Fashion|      1015|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|         Fashion|      1015|\n",
      "|      2|2025-04-17 07:31:...|       like|     Electronics|      1014|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|     Electronics|      1014|\n",
      "|      2|2025-04-17 07:31:...|       like|Health & Fitness|      1013|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|Health & Fitness|      1013|\n",
      "|      2|2025-04-17 07:30:...|add_to_cart|     Accessories|      1012|\n",
      "|      2|2025-04-17 07:30:...|       like|     Electronics|      1011|\n",
      "|      2|2025-04-17 07:30:...|add_to_cart|     Electronics|      1011|\n",
      "|      2|2025-04-17 07:30:...|       like|  Home & Kitchen|      1010|\n",
      "|      2|2025-04-17 07:30:...|add_to_cart|  Home & Kitchen|      1010|\n",
      "|      2|2025-04-17 07:30:...|       like|     Electronics|      1009|\n",
      "+-------+--------------------+-----------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraTest\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"172.22.213.208\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify the loaded JARs\n",
    "print(spark.sparkContext._jsc.sc().listJars())\n",
    "\n",
    "# Attempt to read data from Cassandra\n",
    "try:\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(table=\"user_interactions1\", keyspace=\"ecommerce\") \\\n",
    "        .load()\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a4a2a4",
   "metadata": {},
   "source": [
    "## Storing Mysql data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c61111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark JARs Loaded: C:\\jdbc\\mysql-connector-j-8.4.0\\mysql-connector-j-8.4.0.jar\n",
      "Classpath: None\n",
      "+----+--------------------+--------------------+-------------------+----------------+------+---------+--------------------+------+------------+--------------+-----+------------+--------+\n",
      "|  ID|                Name|               Image|        SubCategory|        Category| Price|    Brand|         Description|Rating|Rating_Count|Stock_Quantity| Size|       Color|Warranty|\n",
      "+----+--------------------+--------------------+-------------------+----------------+------+---------+--------------------+------+------------+--------------+-----+------------+--------+\n",
      "|1001|          Xbox One X|https://m.media-a...|     Gaming Console|     Electronics| 37499|Microsoft|High-performance ...|   4.8|        2981|            50|  N/A|       Black|  1 year|\n",
      "|1002|Philips Sonicare ...|https://m.media-a...|Electric Toothbrush|Health & Fitness|  9749|  Philips|Smart electric to...|   4.7|        2856|           100|  N/A|       White|  1 year|\n",
      "|1003| Nike Training Shoes|https://m.media-a...|     Training Shoes|         Fashion|  6750|     Nike|Durable shoes for...|   4.6|        2330|           200|10 US|       Black|  1 year|\n",
      "|1004|ASUS TUF Gaming L...|https://m.media-a...|      Gaming Laptop|     Electronics|104999|     Asus|Gaming laptop wit...|   4.6|        1938|            30|  N/A|       Black|  1 year|\n",
      "|1005|          Nikon D850|https://m.media-a...|        DSLR Camera|     Electronics|224999|    Nikon|High-resolution D...|   4.9|        1706|            20|  N/A|       Black|  1 year|\n",
      "|1006|Dyson Pure Cool T...|https://m.media-a...|       Air Purifier| Home Appliances| 29999|    Dyson|Air purifier and ...|   4.7|        1277|            50|  N/A|       White| 2 years|\n",
      "|1007|Logitech G Pro Wi...|https://m.media-a...|       Gaming Mouse|     Accessories|  9749| Logitech|Wireless gaming m...|   4.8|        2411|           100|  N/A|       Black|  1 year|\n",
      "|1008|     Apple iPhone 12|https://m.media-a...|        Smartphones|   Mobile Phones| 59999|    Apple|Smartphone with 5...|   4.6|        4423|           300|  N/A|        Blue|  1 year|\n",
      "|1009|    Sony Alpha a6500|https://m.media-a...|  Mirrorless Camera|     Electronics| 89999|     Sony|Mirrorless camera...|   4.7|        1096|            75|  N/A|       Black|  1 year|\n",
      "|1010|Philips Hue White...|https://m.media-a...|     Smart Lighting|  Home & Kitchen| 14999|  Philips|Smart lighting sy...|   4.8|        2293|           150|  N/A|       White|  1 year|\n",
      "|1011|  GoPro Hero 8 Black|https://m.media-a...|      Action Camera|     Electronics| 26249|    GoPro|Action camera wit...|   4.6|        1333|           100|  N/A|       Black|  1 year|\n",
      "|1012|Anker PowerCore 1...|https://m.media-a...|         Power Bank|     Accessories|  1949|    Anker|Compact portable ...|   4.7|        3515|           200|  N/A|       Black|  1 year|\n",
      "|1013|Garmin Forerunner...|https://m.media-a...|         Smartwatch|Health & Fitness| 22499|   Garmin|GPS running watch...|   4.7|        3644|            50|  N/A|       Black|  1 year|\n",
      "|1014|Microsoft Xbox Se...|https://m.media-a...|     Gaming Console|     Electronics| 22499|Microsoft|Compact gaming co...|   4.7|        3094|           120|  N/A|       White|  1 year|\n",
      "|1015| Nike ZoomX Vaporfly|https://m.media-a...|      Running Shoes|         Fashion| 18750|     Nike|High-performance ...|   4.8|        2593|            80|10 US|       White|  1 year|\n",
      "|1016|       OnePlus 9 Pro|https://m.media-a...|         Smartphone|   Mobile Phones| 80249|  OnePlus|Flagship smartpho...|   4.6|        1087|           150|  N/A|Morning Mist|  1 year|\n",
      "|1017|Ember Temperature...|https://m.media-a...|          Smart Mug|  Home & Kitchen|  9746|    Ember|Smart mug with te...|   4.6|        2174|            50|  N/A|       Black|  1 year|\n",
      "|1018|Alienware Aurora ...|https://m.media-a...|          Gaming PC|          Gaming|164999|Alienware|Gaming desktop wi...|   4.8|           9|            20|  N/A|       Black|  1 year|\n",
      "|1019|   Beats Studio Buds|https://m.media-a...|   Wireless Earbuds|     Electronics| 11249|    Beats|True wireless ear...|   4.5|        1543|           200|  N/A|         Red|  1 year|\n",
      "|1020|Apple Magic Keyboard|https://m.media-a...|           Keyboard|     Accessories|  7499|    Apple|Wireless keyboard...|   4.7|        2279|           100|  N/A|       White|  1 year|\n",
      "+----+--------------------+--------------------+-------------------+----------------+------+---------+--------------------+------+------------+--------------+-----+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "jar_path = r\"C:\\jdbc\\mysql-connector-j-8.4.0\\mysql-connector-j-8.4.0.jar\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySQLConnectorTest\") \\\n",
    "    .config(\"spark.jars\", jar_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark JARs Loaded:\", spark.sparkContext._conf.get(\"spark.jars\"))\n",
    "print(\"Classpath:\", spark.sparkContext._conf.get(\"spark.driver.extraClassPath\"))\n",
    "\n",
    "#JDBC connection properties\n",
    "jdbc_hostname = \"127.0.0.1\"\n",
    "jdbc_port = 3306\n",
    "database = \"online_store\"\n",
    "jdbc_url = f\"jdbc:mysql://{jdbc_hostname}:{jdbc_port}/{database}\"\n",
    "connection_properties = {\n",
    "\"user\": \"root\", \n",
    "\"password\": \"220818\",\n",
    "\"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "table = \"product_details\"\n",
    "# Create DataFrame by reading data from the MySQL table\n",
    "try:\n",
    "    df = spark.read.jdbc(url=jdbc_url, table=table, properties=connection_properties)\n",
    "# Show the DataFrame content\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print (f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Set the environment variables for Cassandra connector\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RecommendationSystem\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"172.22.213.208\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 1: Load data from Cassandra and MySQL\n",
    "# Cassandra - User Interactions Data\n",
    "interaction_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"user_interactions1\", keyspace=\"ecommerce\") \\\n",
    "    .load()\n",
    "\n",
    "# MySQL - Product Details Data\n",
    "product_df = spark.read.jdbc(\n",
    "    url=\"jdbc:mysql://127.0.0.1:3306/online_store\",\n",
    "    table=\"product_details\",\n",
    "    properties={\"user\": \"root\", \"password\": \"220818\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    ")\n",
    "\n",
    "# Show the data to verify\n",
    "interaction_df.show(5)\n",
    "product_df.show(5)\n",
    "\n",
    "# Step 2: Prepare the data for recommendation model\n",
    "# Assuming 'user_id' is the user and 'product_id' is the item to be recommended\n",
    "# Create a user-item interaction matrix for ALS model\n",
    "interaction_df = interaction_df.select(\"user_id\", \"product_id\", \"action\").filter(interaction_df.action == \"add_to_cart\")\n",
    "\n",
    "# Convert action to a rating (1 for interaction)\n",
    "interaction_df = interaction_df.withColumn(\"rating\", lit(1))\n",
    "\n",
    "# Step 3: Train ALS model\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"product_id\", ratingCol=\"rating\", nonnegative=True, implicitPrefs=True)\n",
    "model = als.fit(interaction_df)\n",
    "\n",
    "# Step 4: Make recommendations\n",
    "user_recommendations = model.recommendForAllUsers(5)\n",
    "\n",
    "# Show recommendations\n",
    "user_recommendations.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc25de4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+\n",
      "|user_id|product_id|rating    |\n",
      "+-------+----------+----------+\n",
      "|1      |1002      |1.7047042 |\n",
      "|1      |1003      |1.6839231 |\n",
      "|1      |1008      |1.5754317 |\n",
      "|1      |1004      |1.5424463 |\n",
      "|1      |1001      |1.5394212 |\n",
      "|2      |1019      |0.9441399 |\n",
      "|2      |1015      |0.9441399 |\n",
      "|2      |1006      |0.94332683|\n",
      "|2      |1003      |0.940325  |\n",
      "|2      |1008      |0.939277  |\n",
      "|4      |1001      |1.3736395 |\n",
      "|4      |1007      |1.2394998 |\n",
      "|4      |1032      |0.95914793|\n",
      "|4      |1030      |0.95914793|\n",
      "|4      |1028      |0.95914793|\n",
      "|5      |1004      |1.377481  |\n",
      "|5      |1016      |1.2252846 |\n",
      "|5      |1123      |0.9418336 |\n",
      "|5      |1065      |0.9418336 |\n",
      "|5      |1063      |0.9418336 |\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Explode the array of recommendations so each product appears in its own row\n",
    "flattened_recommendations = user_recommendations.withColumn(\"rec\", explode(\"recommendations\")) \\\n",
    "    .select(\"user_id\", \"rec.product_id\", \"rec.rating\")\n",
    "\n",
    "flattened_recommendations.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0cd3019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+--------------+----------+\n",
      "|user_id|    action_timestamp|     action|      category|product_id|\n",
      "+-------+--------------------+-----------+--------------+----------+\n",
      "|      2|2025-04-17 07:31:...|       like|   Electronics|      1019|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|   Electronics|      1019|\n",
      "|      2|2025-04-17 07:31:...|       like|        Gaming|      1018|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|        Gaming|      1018|\n",
      "|      2|2025-04-17 07:31:...|       like|Home & Kitchen|      1017|\n",
      "+-------+--------------------+-----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+--------------------+--------------------+-------------------+----------------+------+---------+--------------------+------+------------+--------------+-----+-----+--------+\n",
      "|  ID|                Name|               Image|        SubCategory|        Category| Price|    Brand|         Description|Rating|Rating_Count|Stock_Quantity| Size|Color|Warranty|\n",
      "+----+--------------------+--------------------+-------------------+----------------+------+---------+--------------------+------+------------+--------------+-----+-----+--------+\n",
      "|1001|          Xbox One X|https://m.media-a...|     Gaming Console|     Electronics| 37499|Microsoft|High-performance ...|   4.8|        2981|            50|  N/A|Black|  1 year|\n",
      "|1002|Philips Sonicare ...|https://m.media-a...|Electric Toothbrush|Health & Fitness|  9749|  Philips|Smart electric to...|   4.7|        2856|           100|  N/A|White|  1 year|\n",
      "|1003| Nike Training Shoes|https://m.media-a...|     Training Shoes|         Fashion|  6750|     Nike|Durable shoes for...|   4.6|        2330|           200|10 US|Black|  1 year|\n",
      "|1004|ASUS TUF Gaming L...|https://m.media-a...|      Gaming Laptop|     Electronics|104999|     Asus|Gaming laptop wit...|   4.6|        1938|            30|  N/A|Black|  1 year|\n",
      "|1005|          Nikon D850|https://m.media-a...|        DSLR Camera|     Electronics|224999|    Nikon|High-resolution D...|   4.9|        1706|            20|  N/A|Black|  1 year|\n",
      "+----+--------------------+--------------------+-------------------+----------------+------+---------+--------------------+------+------------+--------------+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+----------+--------------------+-------------+-----+--------------------+--------------+--------------+--------------------+\n",
      "|user_id|product_id|                Name|     Category|Price|               Image|Product_Rating|Stock_Quantity|         Description|\n",
      "+-------+----------+--------------------+-------------+-----+--------------------+--------------+--------------+--------------------+\n",
      "|      5|      1016|       OnePlus 9 Pro|Mobile Phones|80249|https://m.media-a...|           4.6|           150|Flagship smartpho...|\n",
      "|      4|      1030|Canon RF 50mm F1....|  Electronics|14999|https://m.media-a...|           4.8|            30|Compact prime len...|\n",
      "|      2|      1019|   Beats Studio Buds|  Electronics|11249|https://m.media-a...|           4.5|           200|True wireless ear...|\n",
      "|      5|      1065|Amazon Kindle Pap...|        Books| 9749|https://m.media-a...|           4.6|           100|Waterproof e-read...|\n",
      "|      1|      1008|     Apple iPhone 12|Mobile Phones|59999|https://m.media-a...|           4.6|           300|Smartphone with 5...|\n",
      "+-------+----------+--------------------+-------------+-----+--------------------+--------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Set the environment variables for Cassandra connector\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RecommendationSystem\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"172.22.213.208\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 1: Load data from Cassandra and MySQL\n",
    "# Cassandra - User Interactions Data\n",
    "interaction_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"user_interactions1\", keyspace=\"ecommerce\") \\\n",
    "    .load()\n",
    "\n",
    "# MySQL - Product Details Data\n",
    "product_df = spark.read.jdbc(\n",
    "    url=\"jdbc:mysql://127.0.0.1:3306/online_store\",\n",
    "    table=\"product_details\",\n",
    "    properties={\"user\": \"root\", \"password\": \"220818\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    ")\n",
    "\n",
    "# Show the data to verify\n",
    "interaction_df.show(5)\n",
    "product_df.show(5)\n",
    "\n",
    "# Step 2: Prepare the data for recommendation model\n",
    "# Assuming 'user_id' is the user and 'product_id' is the item to be recommended\n",
    "# Create a user-item interaction matrix for ALS model\n",
    "interaction_df = interaction_df.select(\"user_id\", \"product_id\", \"action\").filter(interaction_df.action == \"add_to_cart\")\n",
    "\n",
    "# Convert action to a rating (1 for interaction)\n",
    "interaction_df = interaction_df.withColumn(\"rating\", lit(1))\n",
    "\n",
    "# Step 3: Train ALS model\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"product_id\", ratingCol=\"rating\", nonnegative=True, implicitPrefs=True)\n",
    "model = als.fit(interaction_df)\n",
    "\n",
    "# Step 4: Make recommendations\n",
    "user_recommendations = model.recommendForAllUsers(5)\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "def get_product_details(user_recommendations, product_df):\n",
    "    # Exploding the recommendations array to create a row for each recommendation\n",
    "    user_recommendations_exploded = user_recommendations.select(\"user_id\", explode(\"recommendations\").alias(\"recommendation\"))\n",
    "    \n",
    "    # Extracting product_id and rating from the exploded recommendations\n",
    "    user_recommendations_exploded = user_recommendations_exploded.select(\n",
    "        \"user_id\", \n",
    "        \"recommendation.product_id\", \n",
    "        \"recommendation.rating\"\n",
    "    )\n",
    "    \n",
    "    # Renaming the 'Rating' column in product_df to avoid ambiguity\n",
    "    product_df_renamed = product_df.withColumnRenamed(\"Rating\", \"Product_Rating\")\n",
    "    \n",
    "    # Now join with the product_df to get details of the recommended products\n",
    "    product_details = user_recommendations_exploded.join(\n",
    "        product_df_renamed, \n",
    "        user_recommendations_exploded.product_id == product_df_renamed.ID\n",
    "    ).select(\n",
    "        \"user_id\", \n",
    "        \"product_id\", \n",
    "        \"Name\", \n",
    "        \"Category\", \n",
    "        \"Price\", \n",
    "        \"Image\", \n",
    "        \"Product_Rating\",  # Use the renamed 'Rating' column\n",
    "        \"Stock_Quantity\", \n",
    "        \"Description\"\n",
    "    )\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Step 6: Display the recommendations with diverse categories\n",
    "product_recommendations = get_product_details(user_recommendations, product_df)\n",
    "\n",
    "# Show the top 5 recommendations\n",
    "product_recommendations.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1fac9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------------+-------------+-----+--------------------------------------------------------------------------+--------------+--------------+-----------------------------------------------+\n",
      "|user_id|product_id|Name                    |Category     |Price|Image                                                                     |Product_Rating|Stock_Quantity|Description                                    |\n",
      "+-------+----------+------------------------+-------------+-----+--------------------------------------------------------------------------+--------------+--------------+-----------------------------------------------+\n",
      "|5      |1016      |OnePlus 9 Pro           |Mobile Phones|80249|https://m.media-amazon.com/images/I/61w-1c1nOzL._AC_UY327_FMwebp_QL65_.jpg|4.6           |150           |Flagship smartphone with Snapdragon 888        |\n",
      "|4      |1030      |Canon RF 50mm F1.8 Lens |Electronics  |14999|https://m.media-amazon.com/images/I/61FX0sONwnL._AC_UY327_FMwebp_QL65_.jpg|4.8           |30            |Compact prime lens for Canon mirrorless cameras|\n",
      "|2      |1019      |Beats Studio Buds       |Electronics  |11249|https://m.media-amazon.com/images/I/51gtvMFl4CL._AC_UY327_FMwebp_QL65_.jpg|4.5           |200           |True wireless earbuds with noise cancellation  |\n",
      "|5      |1065      |Amazon Kindle Paperwhite|Books        |9749 |https://m.media-amazon.com/images/I/61MdbBO+SEL._AC_UY327_FMwebp_QL65_.jpg|4.6           |100           |Waterproof e-reader with 8GB storage           |\n",
      "|1      |1008      |Apple iPhone 12         |Mobile Phones|59999|https://m.media-amazon.com/images/I/71MHTD3uL4L._AC_UY327_FMwebp_QL65_.jpg|4.6           |300           |Smartphone with 5G support                     |\n",
      "+-------+----------+------------------------+-------------+-----+--------------------------------------------------------------------------+--------------+--------------+-----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_recommendations.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875cbaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+--------------+----------+\n",
      "|user_id|    action_timestamp|     action|      category|product_id|\n",
      "+-------+--------------------+-----------+--------------+----------+\n",
      "|      2|2025-04-17 07:31:...|       like|   Electronics|      1019|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|   Electronics|      1019|\n",
      "|      2|2025-04-17 07:31:...|       like|        Gaming|      1018|\n",
      "|      2|2025-04-17 07:31:...|add_to_cart|        Gaming|      1018|\n",
      "|      2|2025-04-17 07:31:...|       like|Home & Kitchen|      1017|\n",
      "+-------+--------------------+-----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+--------------------+--------------------+-------------------+----------------+------+---------+--------------------+------+------------+--------------+-----+-----+--------+\n",
      "|  ID|                Name|               Image|        SubCategory|        Category| Price|    Brand|         Description|Rating|Rating_Count|Stock_Quantity| Size|Color|Warranty|\n",
      "+----+--------------------+--------------------+-------------------+----------------+------+---------+--------------------+------+------------+--------------+-----+-----+--------+\n",
      "|1001|          Xbox One X|https://m.media-a...|     Gaming Console|     Electronics| 37499|Microsoft|High-performance ...|   4.8|        2981|            50|  N/A|Black|  1 year|\n",
      "|1002|Philips Sonicare ...|https://m.media-a...|Electric Toothbrush|Health & Fitness|  9749|  Philips|Smart electric to...|   4.7|        2856|           100|  N/A|White|  1 year|\n",
      "|1003| Nike Training Shoes|https://m.media-a...|     Training Shoes|         Fashion|  6750|     Nike|Durable shoes for...|   4.6|        2330|           200|10 US|Black|  1 year|\n",
      "|1004|ASUS TUF Gaming L...|https://m.media-a...|      Gaming Laptop|     Electronics|104999|     Asus|Gaming laptop wit...|   4.6|        1938|            30|  N/A|Black|  1 year|\n",
      "|1005|          Nikon D850|https://m.media-a...|        DSLR Camera|     Electronics|224999|    Nikon|High-resolution D...|   4.9|        1706|            20|  N/A|Black|  1 year|\n",
      "+----+--------------------+--------------------+-------------------+----------------+------+---------+--------------------+------+------------+--------------+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`user_id`, `action`, `action_timestamp`, `category`, `product_id`].;\n'Filter (datediff(current_date(Some(Asia/Calcutta)), 'timestamp) <= 7)\n+- RelationV2[user_id#0, action_timestamp#1, action#2, category#3, product_id#4]  user_interactions1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m product_df.show(\u001b[32m5\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Step 2: Filter for Recent Interactions (e.g., last 30 days)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m recent_interactions_df = \u001b[43minteraction_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatediff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Only interactions from the last 30 days\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Step 3: Prepare the data for recommendation model\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Assuming 'user_id' is the user and 'product_id' is the item to be recommended\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Create a user-item interaction matrix for ALS model\u001b[39;00m\n\u001b[32m     41\u001b[39m recent_interactions_df = recent_interactions_df.select(\u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m).filter(recent_interactions_df.action == \u001b[33m\"\u001b[39m\u001b[33madd_to_cart\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3331\u001b[39m, in \u001b[36mDataFrame.filter\u001b[39m\u001b[34m(self, condition)\u001b[39m\n\u001b[32m   3329\u001b[39m     jdf = \u001b[38;5;28mself\u001b[39m._jdf.filter(condition)\n\u001b[32m   3330\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[32m-> \u001b[39m\u001b[32m3331\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3332\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3333\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   3334\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN_OR_STR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3335\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcondition\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition).\u001b[34m__name__\u001b[39m},\n\u001b[32m   3336\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srina\\anaconda3\\envs\\fyp\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`user_id`, `action`, `action_timestamp`, `category`, `product_id`].;\n'Filter (datediff(current_date(Some(Asia/Calcutta)), 'timestamp) <= 7)\n+- RelationV2[user_id#0, action_timestamp#1, action#2, category#3, product_id#4]  user_interactions1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, current_date, datediff\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Set the environment variables for Cassandra connector\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RecommendationSystem\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"172.22.213.208\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 1: Load data from Cassandra and MySQL\n",
    "# Cassandra - User Interactions Data\n",
    "interaction_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"user_interactions1\", keyspace=\"ecommerce\") \\\n",
    "    .load()\n",
    "\n",
    "# MySQL - Product Details Data\n",
    "product_df = spark.read.jdbc(\n",
    "    url=\"jdbc:mysql://127.0.0.1:3306/online_store\",\n",
    "    table=\"product_details\",\n",
    "    properties={\"user\": \"root\", \"password\": \"220818\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    ")\n",
    "\n",
    "# Show the data to verify\n",
    "interaction_df.show(5)\n",
    "product_df.show(5)\n",
    "\n",
    "# Step 2: Filter for Recent Interactions (e.g., last 30 days)\n",
    "recent_interactions_df = interaction_df.filter(\n",
    "    datediff(current_date(), col('timestamp')) <= 7  # Only interactions from the last 30 days\n",
    ")\n",
    "\n",
    "# Step 3: Prepare the data for recommendation model\n",
    "# Assuming 'user_id' is the user and 'product_id' is the item to be recommended\n",
    "# Create a user-item interaction matrix for ALS model\n",
    "recent_interactions_df = recent_interactions_df.select(\"user_id\", \"product_id\", \"action\").filter(recent_interactions_df.action == \"add_to_cart\")\n",
    "\n",
    "# Convert action to a rating (1 for interaction)\n",
    "recent_interactions_df = recent_interactions_df.withColumn(\"rating\", lit(1))\n",
    "\n",
    "# Step 4: Train ALS model using recent interactions\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"product_id\", ratingCol=\"rating\", nonnegative=True, implicitPrefs=True)\n",
    "model = als.fit(recent_interactions_df)\n",
    "\n",
    "# Step 5: Make recommendations for users based on recent interactions\n",
    "user_recommendations = model.recommendForAllUsers(5)\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "def get_product_details(user_recommendations, product_df):\n",
    "    # Exploding the recommendations array to create a row for each recommendation\n",
    "    user_recommendations_exploded = user_recommendations.select(\"user_id\", explode(\"recommendations\").alias(\"recommendation\"))\n",
    "    \n",
    "    # Extracting product_id and rating from the exploded recommendations\n",
    "    user_recommendations_exploded = user_recommendations_exploded.select(\n",
    "        \"user_id\", \n",
    "        \"recommendation.product_id\", \n",
    "        \"recommendation.rating\"\n",
    "    )\n",
    "    \n",
    "    # Renaming the 'Rating' column in product_df to avoid ambiguity\n",
    "    product_df_renamed = product_df.withColumnRenamed(\"Rating\", \"Product_Rating\")\n",
    "    \n",
    "    # Now join with the product_df to get details of the recommended products\n",
    "    product_details = user_recommendations_exploded.join(\n",
    "        product_df_renamed, \n",
    "        user_recommendations_exploded.product_id == product_df_renamed.ID\n",
    "    ).select(\n",
    "        \"user_id\", \n",
    "        \"product_id\", \n",
    "        \"Name\", \n",
    "        \"Category\", \n",
    "        \"Price\", \n",
    "        \"Image\", \n",
    "        \"Product_Rating\",  # Use the renamed 'Rating' column\n",
    "        \"Stock_Quantity\", \n",
    "        \"Description\"\n",
    "    )\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Step 6: Display the recommendations with product details\n",
    "product_recommendations = get_product_details(user_recommendations, product_df)\n",
    "\n",
    "# Show the top 5 recommendations\n",
    "product_recommendations.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c879aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
